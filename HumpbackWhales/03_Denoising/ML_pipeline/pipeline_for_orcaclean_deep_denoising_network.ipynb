{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for ORCA-CLEAN Deep Denoizing Network\n",
    "\n",
    "**Requirements** - In order to run this notebook, you will need:\n",
    "- A basic understanding of Machine Learning\n",
    "- An Azure account with an active subscription - [Create an account for free](https://azure.microsoft.com/free/?WT.mc_id=A261C142F)\n",
    "- An Azure ML workspace with computer cluster - [Configure workspace](../../configuration.ipynb)\n",
    "- A python environment\n",
    "- Installed Azure Machine Learning Python SDK v2 - [install instructions](../../../README.md) - check the getting started section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Connect to Azure Machine Learning Workspace\n",
    "\n",
    "The [workspace](https://docs.microsoft.com/en-us/azure/machine-learning/concept-workspace) is the top-level resource for Azure Machine Learning, providing a centralized place to work with all the artifacts you create when you use Azure Machine Learning. In this section we will connect to the workspace in which the job will be run.\n",
    "\n",
    "## 1.1 Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "from azure.ai.ml import MLClient, Input, Output\n",
    "from azure.ai.ml.dsl import pipeline\n",
    "from azure.ai.ml import load_component\n",
    "from azure.ai.ml.entities import AmlCompute, Environment, Model\n",
    "from azure.ai.ml.constants import AssetTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Configure credential\n",
    "\n",
    "We are using `DefaultAzureCredential` to get access to workspace. \n",
    "`DefaultAzureCredential` should be capable of handling most Azure SDK authentication scenarios. \n",
    "\n",
    "Reference for more available credentials if it does not work for you: [configure credential example](../../configuration.ipynb), [azure-identity reference doc](https://docs.microsoft.com/en-us/python/api/azure-identity/azure.identity?view=azure-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Get a handle to the workspace\n",
    "\n",
    "We use config file to connect to a workspace. The Azure ML workspace should be configured with computer cluster. [Check this notebook for configure a workspace](../../configuration.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: /config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enable_node_public_ip: true\n",
      "id: /subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourceGroups/lianatests/providers/Microsoft.MachineLearningServices/workspaces/humpbackwhales-aml/computes/whalesdenoisinggpu2\n",
      "idle_time_before_scale_down: 180\n",
      "location: westeurope\n",
      "max_instances: 4\n",
      "min_instances: 0\n",
      "name: whalesdenoisinggpu2\n",
      "network_settings: {}\n",
      "provisioning_state: Succeeded\n",
      "size: STANDARD_NC24ADS_A100_V4\n",
      "ssh_public_access_enabled: true\n",
      "tier: dedicated\n",
      "type: amlcompute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get a handle to workspace\n",
    "ml_client = MLClient.from_config(credential=credential)\n",
    "\n",
    "compute_instance_name = \"whalesdenoisinggpu2\"\n",
    "try:\n",
    "    print(ml_client.compute.get(compute_instance_name))\n",
    "except Exception as ex:\n",
    "    compute_instance = AmlCompute(\n",
    "        name=compute_instance_name,\n",
    "        type=\"amlcompute\",\n",
    "        size=\"Standard_NC24ads_A100_v4\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=0,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "    ml_client.begin_create_or_update(compute_instance).result()\n",
    "    print(f\"Created new compute instance: {ml_client.compute.get(compute_instance_name)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Create a custom environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name whales-denoising-env is registered to workspace, the environment version is 1.3\n"
     ]
    }
   ],
   "source": [
    "custom_env_name = \"whales-denoising-env\"\n",
    "version = \"1.3\"\n",
    "dependencies_dir = \"./dependencies\"\n",
    "\n",
    "try:\n",
    "    pipeline_job_env = ml_client.environments.get(name=custom_env_name, version=version)\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )\n",
    "except:\n",
    "    pipeline_job_env = Environment(\n",
    "        name=custom_env_name,\n",
    "        description=\"Custom environment for running ORCA-CLEAN pipeline\",\n",
    "        conda_file=os.path.join(dependencies_dir, \"conda.yaml\"),\n",
    "        image=\"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04\", #\"mcr.microsoft.com/azureml/curated/acpt-pytorch-1.13-cuda11.7:18\",\n",
    "        version=version,\n",
    "    )\n",
    "\n",
    "    pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "    print(\n",
    "        f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define and create components into workspace\n",
    "## 2.1 Load components from YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = \".\"\n",
    "train_model = load_component(source=parent_dir + \"/train_model.yml\")\n",
    "#score_data = load_component(source=parent_dir + \"/score_data.yml\")\n",
    "#eval_model = load_component(source=parent_dir + \"/eval_model.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Inspect loaded component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "name: train_model\n",
      "version: 0.0.1\n",
      "display_name: Train Deep Denoising Model\n",
      "description: The training component\n",
      "type: command\n",
      "inputs:\n",
      "  whales_data:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  noise_train_data:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  noise_val_data:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  noise_test_data:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  max_train_epochs:\n",
      "    type: integer\n",
      "    default: '150'\n",
      "  learning_rate:\n",
      "    type: number\n",
      "    default: '0.001'\n",
      "  batch_size:\n",
      "    type: integer\n",
      "    default: '16'\n",
      "  num_workers:\n",
      "    type: integer\n",
      "    default: '6'\n",
      "  augmentation:\n",
      "    type: integer\n",
      "    default: '1'\n",
      "  n_fft:\n",
      "    type: integer\n",
      "    default: '4096'\n",
      "  hop_length:\n",
      "    type: integer\n",
      "    default: '441'\n",
      "  sequence_len:\n",
      "    type: integer\n",
      "    default: '1280'\n",
      "  freq_compression:\n",
      "    type: string\n",
      "    default: linear\n",
      "  model_dir:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  log_dir:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  checkpoint_dir:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  cache_dir:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "  summary_dir:\n",
      "    type: uri_folder\n",
      "    mode: rw_mount\n",
      "outputs:\n",
      "  model_output:\n",
      "    type: uri_folder\n",
      "command: python main.py  --debug  --random_val --max_train_epochs ${{inputs.max_train_epochs}}  --lr\n",
      "  ${{inputs.learning_rate}}  --batch_size ${{inputs.batch_size}}  --num_workers ${{inputs.num_workers}}  --data_dir\n",
      "  ${{inputs.whales_data}}  --noise_dir_train ${{inputs.noise_train_data}}  --noise_dir_val\n",
      "  ${{inputs.noise_val_data}}  --noise_dir_test ${{inputs.noise_test_data}}  --model_dir\n",
      "  ${{inputs.model_dir}}  --log_dir ${{inputs.log_dir}}  --checkpoint_dir ${{inputs.checkpoint_dir}}  --cache_dir\n",
      "  ${{inputs.cache_dir}}  --summary_dir ${{inputs.summary_dir}}  --augmentation ${{inputs.augmentation}}  --n_fft\n",
      "  ${{inputs.n_fft}}  --hop_length ${{inputs.hop_length}}  --sequence_len ${{inputs.sequence_len}}  --freq_compression\n",
      "  ${{inputs.freq_compression}}\n",
      "environment: azureml:whales-denoising-env:1.3\n",
      "code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/ML_pipeline/orca_clean\n",
      "is_deterministic: true\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the component as yaml\n",
    "print(train_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Pipeline job\n",
    "## 3.1 Build pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct pipeline\n",
    "@pipeline()\n",
    "def pipeline_with_components_from_yaml(\n",
    "    whales_data_input,\n",
    "    noise_train_data_input,\n",
    "    noise_val_data_input,\n",
    "    noise_test_data_input,\n",
    "    model_dir_output,\n",
    "    log_dir_output,\n",
    "    checkpoint_dir_output,\n",
    "    cache_dir_output,\n",
    "    summary_dir_output,\n",
    "    max_train_epochs,\n",
    "    \n",
    "):\n",
    "    \"\"\"Pipeline with components defined via yaml.\"\"\"\n",
    "    train_step = train_model(\n",
    "        whales_data=whales_data_input,\n",
    "        noise_train_data=noise_train_data_input,\n",
    "        noise_val_data=noise_val_data_input,\n",
    "        noise_test_data=noise_test_data_input,\n",
    "        model_dir=model_dir_output,\n",
    "        log_dir=log_dir_output,\n",
    "        checkpoint_dir=checkpoint_dir_output,\n",
    "        cache_dir=cache_dir_output,\n",
    "        summary_dir=summary_dir_output,\n",
    "        max_train_epochs=max_train_epochs\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"trained_model\": train_step.outputs.model_output\n",
    "    }\n",
    "\n",
    "mounted_dir = \"/mnt/humpbackwhales/data/denoising_data\"\n",
    "mounter_output_dir = \"/mnt/humpbackwhales/denoising/ML_pipeline\"\n",
    "#mounted_dir = azureml://datastores/workspaceblobstore/paths/tutorial-datasets/places2/train/\n",
    "\n",
    "pipeline_job = pipeline_with_components_from_yaml(\n",
    "    whales_data_input=Input(type=\"uri_folder\", path=mounted_dir + \"/whales_data_v2/\"),\n",
    "    noise_train_data_input=Input(type=\"uri_folder\", path=mounted_dir + \"/noise_train_v2/\"),\n",
    "    noise_val_data_input=Input(type=\"uri_folder\", path=mounted_dir + \"/noise_val_v2/\"),\n",
    "    noise_test_data_input=Input(type=\"uri_folder\", path=mounted_dir + \"/noise_test_v2/\"),\n",
    "    model_dir_output=Input(type=\"uri_folder\", path=mounter_output_dir + \"/models/\"),\n",
    "    log_dir_output=Input(type=\"uri_folder\", path=mounter_output_dir + \"/logs/\"),\n",
    "    checkpoint_dir_output=Input(type=\"uri_folder\", path=mounter_output_dir + \"/model_checkpoints/\"),\n",
    "    cache_dir_output=Input(type=\"uri_folder\", path=mounter_output_dir + \"/cache/\"),\n",
    "    summary_dir_output=Input(type=\"uri_folder\", path=mounter_output_dir + \"/summary/\"),\n",
    "    max_train_epochs=5,\n",
    ")\n",
    "\n",
    "# set pipeline level compute\n",
    "pipeline_job.settings.default_compute = compute_instance_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_name: pipeline_with_components_from_yaml\n",
      "description: Pipeline with components defined via yaml.\n",
      "type: pipeline\n",
      "inputs:\n",
      "  whales_data_input:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/data/denoising_data/whales_data_v2/\n",
      "  noise_train_data_input:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/data/denoising_data/noise_train_v2/\n",
      "  noise_val_data_input:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/data/denoising_data/noise_val_v2/\n",
      "  noise_test_data_input:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/data/denoising_data/noise_test_v2/\n",
      "  model_dir_output:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/denoising/ML_pipeline/models/\n",
      "  log_dir_output:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/denoising/ML_pipeline/logs/\n",
      "  checkpoint_dir_output:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/denoising/ML_pipeline/model_checkpoints/\n",
      "  cache_dir_output:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/denoising/ML_pipeline/cache/\n",
      "  summary_dir_output:\n",
      "    type: uri_folder\n",
      "    path: azureml:/mnt/humpbackwhales/denoising/ML_pipeline/summary/\n",
      "  max_train_epochs: 5\n",
      "outputs:\n",
      "  trained_model:\n",
      "    type: uri_folder\n",
      "jobs:\n",
      "  train_step:\n",
      "    type: command\n",
      "    inputs:\n",
      "      whales_data:\n",
      "        path: ${{parent.inputs.whales_data_input}}\n",
      "      noise_train_data:\n",
      "        path: ${{parent.inputs.noise_train_data_input}}\n",
      "      noise_val_data:\n",
      "        path: ${{parent.inputs.noise_val_data_input}}\n",
      "      noise_test_data:\n",
      "        path: ${{parent.inputs.noise_test_data_input}}\n",
      "      max_train_epochs:\n",
      "        path: ${{parent.inputs.max_train_epochs}}\n",
      "      model_dir:\n",
      "        path: ${{parent.inputs.model_dir_output}}\n",
      "      log_dir:\n",
      "        path: ${{parent.inputs.log_dir_output}}\n",
      "      checkpoint_dir:\n",
      "        path: ${{parent.inputs.checkpoint_dir_output}}\n",
      "      cache_dir:\n",
      "        path: ${{parent.inputs.cache_dir_output}}\n",
      "      summary_dir:\n",
      "        path: ${{parent.inputs.summary_dir_output}}\n",
      "    outputs:\n",
      "      model_output: ${{parent.outputs.trained_model}}\n",
      "    component:\n",
      "      $schema: https://azuremlschemas.azureedge.net/latest/commandComponent.schema.json\n",
      "      name: train_model\n",
      "      version: 0.0.1\n",
      "      display_name: Train Deep Denoising Model\n",
      "      description: The training component\n",
      "      type: command\n",
      "      inputs:\n",
      "        whales_data:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        noise_train_data:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        noise_val_data:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        noise_test_data:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        max_train_epochs:\n",
      "          type: integer\n",
      "          default: '150'\n",
      "        learning_rate:\n",
      "          type: number\n",
      "          default: '0.001'\n",
      "        batch_size:\n",
      "          type: integer\n",
      "          default: '16'\n",
      "        num_workers:\n",
      "          type: integer\n",
      "          default: '6'\n",
      "        augmentation:\n",
      "          type: integer\n",
      "          default: '1'\n",
      "        n_fft:\n",
      "          type: integer\n",
      "          default: '4096'\n",
      "        hop_length:\n",
      "          type: integer\n",
      "          default: '441'\n",
      "        sequence_len:\n",
      "          type: integer\n",
      "          default: '1280'\n",
      "        freq_compression:\n",
      "          type: string\n",
      "          default: linear\n",
      "        model_dir:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        log_dir:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        checkpoint_dir:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        cache_dir:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "        summary_dir:\n",
      "          type: uri_folder\n",
      "          mode: rw_mount\n",
      "      outputs:\n",
      "        model_output:\n",
      "          type: uri_folder\n",
      "      command: python main.py  --debug  --random_val --max_train_epochs ${{inputs.max_train_epochs}}  --lr\n",
      "        ${{inputs.learning_rate}}  --batch_size ${{inputs.batch_size}}  --num_workers\n",
      "        ${{inputs.num_workers}}  --data_dir ${{inputs.whales_data}}  --noise_dir_train\n",
      "        ${{inputs.noise_train_data}}  --noise_dir_val ${{inputs.noise_val_data}}  --noise_dir_test\n",
      "        ${{inputs.noise_test_data}}  --model_dir ${{inputs.model_dir}}  --log_dir\n",
      "        ${{inputs.log_dir}}  --checkpoint_dir ${{inputs.checkpoint_dir}}  --cache_dir\n",
      "        ${{inputs.cache_dir}}  --summary_dir ${{inputs.summary_dir}}  --augmentation\n",
      "        ${{inputs.augmentation}}  --n_fft ${{inputs.n_fft}}  --hop_length ${{inputs.hop_length}}  --sequence_len\n",
      "        ${{inputs.sequence_len}}  --freq_compression ${{inputs.freq_compression}}\n",
      "      environment: azureml:whales-denoising-env:1.3\n",
      "      code: /mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/ML_pipeline/orca_clean\n",
      "      is_deterministic: true\n",
      "settings:\n",
      "  default_compute: azureml:whalesdenoisinggpu2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect built pipeline\n",
    "print(pipeline_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Submit pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/humpbackwhales/data/denoising_data/whales_data_v2/' 'https://humpbackwhalessa.blob.core.windows.net/azureml-blobstore-f4e80658-2a40-4426-8676-c5538afa6a8f/LocalUpload/c4f8edf663ec11fe0d34c4808ef63531/whales_data_v2' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n",
      "Your file exceeds 100 MB. If you experience low speeds, latency, or broken connections, we recommend using the AzCopyv10 tool for this file transfer.\n",
      "\n",
      "Example: azcopy copy '/mnt/humpbackwhales/data/denoising_data/noise_train_v2/' 'https://humpbackwhalessa.blob.core.windows.net/azureml-blobstore-f4e80658-2a40-4426-8676-c5538afa6a8f/LocalUpload/2a9e34de5512ddd07b23092d3f2dd598/noise_train_v2' \n",
      "\n",
      "See https://docs.microsoft.com/azure/storage/common/storage-use-azcopy-v10 for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>whales_denoising</td><td>jovial_steelpan_gcmh21z3s2</td><td>pipeline</td><td>Preparing</td><td><a href=\"https://ml.azure.com/runs/jovial_steelpan_gcmh21z3s2?wsid=/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourcegroups/lianatests/workspaces/humpbackwhales-aml&amp;tid=16b3c013-d300-468d-ac64-7eda0820b6d3\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {'whales_data_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03fa0>, 'noise_train_data_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc034c0>, 'noise_val_data_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc035b0>, 'noise_test_data_input': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03190>, 'model_dir_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03490>, 'log_dir_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03e80>, 'checkpoint_dir_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03d30>, 'cache_dir_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03fd0>, 'summary_dir_output': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03e50>, 'max_train_epochs': <azure.ai.ml.entities._job.pipeline._io.base.PipelineInput object at 0x7f4e4cc03c40>}, 'outputs': {'trained_model': <azure.ai.ml.entities._job.pipeline._io.base.PipelineOutput object at 0x7f4e4cc03e20>}, 'jobs': {}, 'component': PipelineComponent({'intellectual_property': None, 'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'auto_delete_setting': None, 'name': 'azureml_anonymous', 'description': 'Pipeline with components defined via yaml.', 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/ML_pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f4e4cc2c1c0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'pipeline_with_components_from_yaml', 'is_deterministic': None, 'inputs': {'whales_data_input': {}, 'noise_train_data_input': {}, 'noise_val_data_input': {}, 'noise_test_data_input': {}, 'model_dir_output': {}, 'log_dir_output': {}, 'checkpoint_dir_output': {}, 'cache_dir_output': {}, 'summary_dir_output': {}, 'max_train_epochs': {}}, 'outputs': {'trained_model': {}}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {'train_step': Command({'parameters': {}, 'init': False, 'name': 'train_step', 'type': 'command', 'status': None, 'log_files': None, 'description': None, 'tags': {}, 'properties': {}, 'print_as_yaml': True, 'id': None, 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/ML_pipeline', 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f4e4cc2cfa0>, 'allowed_keys': {}, 'key_restriction': False, 'logger': <Logger attr_dict (WARNING)>, 'display_name': None, 'experiment_name': None, 'compute': None, 'services': None, 'comment': None, 'job_inputs': {'whales_data': '${{parent.inputs.whales_data_input}}', 'noise_train_data': '${{parent.inputs.noise_train_data_input}}', 'noise_val_data': '${{parent.inputs.noise_val_data_input}}', 'noise_test_data': '${{parent.inputs.noise_test_data_input}}', 'max_train_epochs': '${{parent.inputs.max_train_epochs}}', 'model_dir': '${{parent.inputs.model_dir_output}}', 'log_dir': '${{parent.inputs.log_dir_output}}', 'checkpoint_dir': '${{parent.inputs.checkpoint_dir_output}}', 'cache_dir': '${{parent.inputs.cache_dir_output}}', 'summary_dir': '${{parent.inputs.summary_dir_output}}'}, 'job_outputs': {'model_output': '${{parent.outputs.trained_model}}'}, 'inputs': {'whales_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2caf0>, 'noise_train_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2ca60>, 'noise_val_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2cc10>, 'noise_test_data': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2c850>, 'max_train_epochs': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2ce20>, 'model_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2c220>, 'log_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2c2e0>, 'checkpoint_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2c3d0>, 'cache_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2ce50>, 'summary_dir': <azure.ai.ml.entities._job.pipeline._io.base.NodeInput object at 0x7f4e4cc2c5b0>}, 'outputs': {'model_output': <azure.ai.ml.entities._job.pipeline._io.base.NodeOutput object at 0x7f4e4cc2c970>}, 'component': 'azureml_anonymous:710b68b7-e7cb-4ee2-8d99-e164ae21fe08', 'referenced_control_flow_node_instance_id': None, 'kwargs': {'services': None}, 'instance_id': '220fe198-5702-4947-b36f-ff168ac082fe', 'source': 'REMOTE.WORKSPACE.COMPONENT', 'validate_required_input_not_provided': True, 'limits': None, 'identity': None, 'distribution': None, 'environment_variables': {}, 'environment': None, 'resources': None, 'queue_settings': None, 'swept': False})}, 'job_types': {'command': 1}, 'job_sources': {'REMOTE.WORKSPACE.COMPONENT': 1}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Preparing', 'log_files': None, 'name': 'jovial_steelpan_gcmh21z3s2', 'description': 'Pipeline with components defined via yaml.', 'tags': {}, 'properties': {'azureml.DevPlatv2': 'true', 'azureml.DatasetAccessMode': 'Asset', 'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'MFE', 'runType': 'HTTP', 'azureml.parameters': '{\"max_train_epochs\":\"5\"}', 'azureml.continue_on_step_failure': 'True', 'azureml.continue_on_failed_optional_input': 'True', 'azureml.enforceRerun': 'False', 'azureml.defaultComputeName': 'whalesdenoisinggpu2', 'azureml.defaultDataStoreName': 'workspaceblobstore', 'azureml.pipelineComponent': 'pipelinerun'}, 'print_as_yaml': True, 'id': '/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourceGroups/lianatests/providers/Microsoft.MachineLearningServices/workspaces/humpbackwhales-aml/jobs/jovial_steelpan_gcmh21z3s2', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/linapalk2/code/Users/linapalk/ML_pipeline', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f4e4cc03ca0>, 'serialize': <msrest.serialization.Serializer object at 0x7f4e4cc02f20>, 'display_name': 'pipeline_with_components_from_yaml', 'experiment_name': 'whales_denoising', 'compute': None, 'services': {'Tracking': {'endpoint': 'azureml://westeurope.api.azureml.ms/mlflow/v1.0/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourceGroups/lianatests/providers/Microsoft.MachineLearningServices/workspaces/humpbackwhales-aml?', 'type': 'Tracking'}, 'Studio': {'endpoint': 'https://ml.azure.com/runs/jovial_steelpan_gcmh21z3s2?wsid=/subscriptions/03fd01f6-6051-4545-a78e-ceaace399b96/resourcegroups/lianatests/workspaces/humpbackwhales-aml&tid=16b3c013-d300-468d-ac64-7eda0820b6d3', 'type': 'Studio'}}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit pipeline job to workspace\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline_job, experiment_name=\"whales_denoising\"\n",
    ")\n",
    "pipeline_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the job completes\n",
    "ml_client.jobs.stream(pipeline_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Register a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(UserError) The request is invalid.\nCode: UserError\nMessage: The request is invalid.\nException Details:\t(NoMatchingArtifactsFoundFromJob) No artifacts matching model found from Job.\n\tCode: NoMatchingArtifactsFoundFromJob\n\tMessage: No artifacts matching model found from Job.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"684650c9348bd0465b71ae6d4ce830e1\",\n        \"request\": \"7df8c33f08f3e5ed\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Location\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Time\nInfo: {\n    \"value\": \"2023-09-15T09:06:04.6361655+00:00\"\n}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      4\u001b[0m pipeline_job_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclever_horse_5ykpzjl7sp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m run_model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m      7\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureml://jobs/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/outputs/artifacts/paths/model/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(pipeline_job_name), \u001b[38;5;66;03m# pipeline_job.name\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morca-clean-model-v1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel trained with 5 epochs, whales_data_v2 and noise_train/test/val_v2.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mAssetTypes\u001b[38;5;241m.\u001b[39mMLFLOW_MODEL\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:235\u001b[0m, in \u001b[0;36mModelOperations.create_or_update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    233\u001b[0m     log_and_raise_error(ex)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:223\u001b[0m, in \u001b[0;36mModelOperations.create_or_update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m ASSET_PATH_ERROR:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AssetPathException(\n\u001b[1;32m    218\u001b[0m             message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG,\n\u001b[1;32m    219\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mMODEL,\n\u001b[1;32m    220\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG_NO_PERSONAL_DATA,\n\u001b[1;32m    221\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    222\u001b[0m         )\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    225\u001b[0m model \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39m_from_rest_object(result)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_increment_version \u001b[38;5;129;01mand\u001b[39;00m indicator_file:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:202\u001b[0m, in \u001b[0;36mModelOperations.create_or_update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    191\u001b[0m auto_increment_version \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_auto_increment_version\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_versions_operation\u001b[38;5;241m.\u001b[39mbegin_create_or_update(\n\u001b[1;32m    195\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    196\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    197\u001b[0m             body\u001b[38;5;241m=\u001b[39mmodel_version_resource,\n\u001b[1;32m    198\u001b[0m             registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name,\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs,\n\u001b[1;32m    200\u001b[0m         )\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_versions_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scope_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name:\n\u001b[1;32m    212\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname, version\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mversion)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_model_versions_operations.py:650\u001b[0m, in \u001b[0;36mModelVersionsOperations.create_or_update\u001b[0;34m(self, resource_group_name, workspace_name, name, version, body, **kwargs)\u001b[0m\n\u001b[1;32m    648\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m    649\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[0;32m--> 650\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m    653\u001b[0m     deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModelVersion\u001b[39m\u001b[38;5;124m'\u001b[39m, pipeline_response)\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: (UserError) The request is invalid.\nCode: UserError\nMessage: The request is invalid.\nException Details:\t(NoMatchingArtifactsFoundFromJob) No artifacts matching model found from Job.\n\tCode: NoMatchingArtifactsFoundFromJob\n\tMessage: No artifacts matching model found from Job.\nAdditional Information:Type: ComponentName\nInfo: {\n    \"value\": \"managementfrontend\"\n}Type: Correlation\nInfo: {\n    \"value\": {\n        \"operation\": \"684650c9348bd0465b71ae6d4ce830e1\",\n        \"request\": \"7df8c33f08f3e5ed\"\n    }\n}Type: Environment\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Location\nInfo: {\n    \"value\": \"westeurope\"\n}Type: Time\nInfo: {\n    \"value\": \"2023-09-15T09:06:04.6361655+00:00\"\n}"
     ]
    }
   ],
   "source": [
    "run_model = Model(\n",
    "    path=\"azureml://jobs/{}/outputs/artifacts/paths/model/\".format(pipeline_job.name),\n",
    "    name=\"orca-clean-model-v1\",\n",
    "    description=\"Model trained with 5 epochs, whales_data_v2 and noise_train/test/val_v2.\",\n",
    "    type=AssetTypes.MLFLOW_MODEL\n",
    ")\n",
    "\n",
    "ml_client.models.create_or_update(run_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Create a batch endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import BatchEndpoint, ModelBatchDeployment, PipelineComponentBatchDeployment\n",
    "\n",
    "endpoint_name=\"whales-denoising-batch\"\n",
    "\n",
    "endpoint = BatchEndpoint(\n",
    "    name=endpoint_name,\n",
    "    description=\"Endpoint for pipeline deployments\",\n",
    ")\n",
    "\n",
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()\n",
    "\n",
    "endpoint = ml_client.batch_endpoints.get(name=endpoint_name)\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Deploy the pipeline job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment = PipelineComponentBatchDeployment(\n",
    "    name=\"whales-denoising-batch-from-job\",\n",
    "    description=\"This deployment is created from a pipeline job.\",\n",
    "    endpoint_name=endpoint.name,\n",
    "    job_definition=pipeline_job,\n",
    "    settings={\n",
    "        \"default_compute\": compute_instance_name,\n",
    "        \"continue_on_step_failure\": False\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.batch_deployments.begin_create_or_update(deployment).result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = ml_client.batch_endpoints.get(endpoint.name)\n",
    "endpoint.defaults.deployment_name = deployment.name\n",
    "ml_client.batch_endpoints.begin_create_or_update(endpoint).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Test the deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job = ml_client.batch_endpoints.invoke(\n",
    "    endpoint_name=endpoint.name, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.jobs.get(name=job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "description": {
   "description": "Create pipeline with CommandComponents from local YAML file"
  },
  "interpreter": {
   "hash": "3e9e0e270b75c5e6da2e22113ba4f77b864d68f95da6601809c29e46c73ae6bb"
  },
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
